{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DPue4B2vPu2s",
    "outputId": "479fd42c-3038-4eb4-af06-3e53abc1ea45"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "3iDlvtrBQZO3",
    "outputId": "16c19bd5-6647-437d-b9f0-cdbaffd167a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0.dev20200911\n",
      "0.8.0.dev20200911\n"
     ]
    }
   ],
   "source": [
    "# check PyTorch versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's available : 1\n",
      "GPU device name : GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "    print(f\"Number of GPU's available : {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU device name : {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vvY6qsJ0Qy5n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use standard FashionMNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = len(train_set), num_workers = 1, pin_memory = True\n",
    ")\n",
    "data = next(iter(loader))\n",
    "mean = data[0].mean()\n",
    "std = data[0].std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_normal = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_classes = len(train_set.classes)\n",
    "out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets = {\n",
    "    'not_normal': train_set,\n",
    "    'normal' : train_set_normal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rZPckqWTRXUm"
   },
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "# class Network(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # define layers\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "#         self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "#         self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "#         # define forward function\n",
    "#     def forward(self, t):\n",
    "#         # conv 1\n",
    "#         t = self.conv1(t)\n",
    "#         t = F.relu(t)\n",
    "#         t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "#         # conv 2\n",
    "#         t = self.conv2(t)\n",
    "#         t = F.relu(t)\n",
    "#         t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "#         # fc1\n",
    "#         t = t.reshape(-1, 12*4*4)\n",
    "#         t = self.fc1(t)\n",
    "#         t = F.relu(t)\n",
    "\n",
    "#         # fc2\n",
    "#         t = self.fc2(t)\n",
    "#         t = F.relu(t)\n",
    "\n",
    "#         # output\n",
    "#         t = self.out(t)\n",
    "#         # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "#         return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "Network1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.Flatten(start_dim = 1),\n",
    "    nn.Linear(in_features = 12*4*4, out_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 120, out_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 60, out_features = 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "#  BatchNormalization computes statistics only with respect to a single axis \n",
    "#  (usually the channels axis, =-1 (last) by default); \n",
    "#  every other axis is collapsed, i.e. summed over for averaging.\n",
    "NetworkBN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.BatchNorm2d(6),\n",
    "    nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.Flatten(start_dim = 1),\n",
    "    # 193 * 120 = num param\n",
    "    nn.Linear(in_features = 12*4*4, out_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Linear(in_features = 120, out_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 60, out_features = 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Flatten(start_dim=1, end_dim=-1)\n",
      "  (8): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (12): ReLU()\n",
      "  (13): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(NetworkBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [128, 6, 24, 24]             156\n",
      "              ReLU-2           [128, 6, 24, 24]               0\n",
      "         MaxPool2d-3           [128, 6, 12, 12]               0\n",
      "       BatchNorm2d-4           [128, 6, 12, 12]              12\n",
      "            Conv2d-5            [128, 12, 8, 8]           1,812\n",
      "              ReLU-6            [128, 12, 8, 8]               0\n",
      "         MaxPool2d-7            [128, 12, 4, 4]               0\n",
      "           Flatten-8                 [128, 192]               0\n",
      "            Linear-9                 [128, 120]          23,160\n",
      "             ReLU-10                 [128, 120]               0\n",
      "      BatchNorm1d-11                 [128, 120]             240\n",
      "           Linear-12                  [128, 60]           7,260\n",
      "             ReLU-13                  [128, 60]               0\n",
      "           Linear-14                  [128, 10]             610\n",
      "================================================================\n",
      "Total params: 33,250\n",
      "Trainable params: 33,250\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 10.79\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 11.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkBN.to(device)\n",
    "summary(model, input_size=(1, 28, 28), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "# layer normalization by computing the mean and variance used for normalization from all of the summed inputs \n",
    "# to the neurons in a layer on a single training case\n",
    "NetworkLN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.LayerNorm([6,12,12]),\n",
    "    nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.Flatten(start_dim = 1),\n",
    "    nn.Linear(in_features = 12*4*4, out_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.LayerNorm([120]),\n",
    "    nn.Linear(in_features = 120, out_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 60, out_features = 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [128, 6, 24, 24]             156\n",
      "              ReLU-2           [128, 6, 24, 24]               0\n",
      "         MaxPool2d-3           [128, 6, 12, 12]               0\n",
      "         LayerNorm-4           [128, 6, 12, 12]           1,728\n",
      "            Conv2d-5            [128, 12, 8, 8]           1,812\n",
      "              ReLU-6            [128, 12, 8, 8]               0\n",
      "         MaxPool2d-7            [128, 12, 4, 4]               0\n",
      "           Flatten-8                 [128, 192]               0\n",
      "            Linear-9                 [128, 120]          23,160\n",
      "             ReLU-10                 [128, 120]               0\n",
      "        LayerNorm-11                 [128, 120]             240\n",
      "           Linear-12                  [128, 60]           7,260\n",
      "             ReLU-13                  [128, 60]               0\n",
      "           Linear-14                  [128, 10]             610\n",
      "================================================================\n",
      "Total params: 34,966\n",
      "Trainable params: 34,966\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 10.79\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 11.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkLN.to(device)\n",
    "summary(model, input_size=(1, 28, 28), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "#  InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. \n",
    "# InstanceNorm2d is applied on each channel of channeled data like RGB images, \n",
    "# but LayerNorm is usually applied on entire sample and often in NLP tasks.\n",
    "# Additionally, LayerNorm applies elementwise affine transform, \n",
    "# while InstanceNorm2d usually don’t apply affine transform.\n",
    "NetworkIN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    # num_features – C from an expected input of size (N,C,H,W)\n",
    "    nn.InstanceNorm2d(6, affine=True),\n",
    "    nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.InstanceNorm2d(12, affine=True),\n",
    "    nn.Flatten(start_dim = 1),\n",
    "    nn.Linear(in_features = 12*4*4, out_features = 120),\n",
    "    nn.ReLU(),\n",
    "    # doesn't work InstanceNorm1d returns 0-filled tensor to 2D tensor.\n",
    "    # This is because InstanceNorm1d reshapes inputs to(1, N * C, ...) \n",
    "    # from (N, C,...) and this makesvariances 0.\n",
    "    # https://github.com/pytorch/pytorch/issues/11991\n",
    "    # nn.InstanceNorm1d(120, affine=True),\n",
    "    nn.Linear(in_features = 120, out_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 60, out_features = 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [128, 6, 24, 24]             156\n",
      "              ReLU-2           [128, 6, 24, 24]               0\n",
      "         MaxPool2d-3           [128, 6, 12, 12]               0\n",
      "    InstanceNorm2d-4           [128, 6, 12, 12]              12\n",
      "            Conv2d-5            [128, 12, 8, 8]           1,812\n",
      "              ReLU-6            [128, 12, 8, 8]               0\n",
      "         MaxPool2d-7            [128, 12, 4, 4]               0\n",
      "    InstanceNorm2d-8            [128, 12, 4, 4]              24\n",
      "           Flatten-9                 [128, 192]               0\n",
      "           Linear-10                 [128, 120]          23,160\n",
      "             ReLU-11                 [128, 120]               0\n",
      "           Linear-12                  [128, 60]           7,260\n",
      "             ReLU-13                  [128, 60]               0\n",
      "           Linear-14                  [128, 10]             610\n",
      "================================================================\n",
      "Total params: 33,034\n",
      "Trainable params: 33,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 10.86\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 11.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkIN.to(device)\n",
    "summary(model, input_size=(1, 28, 28), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> input = torch.randn(20, 6, 10, 10)\n",
    "# >>> # Separate 6 channels into 3 groups\n",
    "# >>> m = nn.GroupNorm(3, 6)\n",
    "# >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n",
    "# >>> m = nn.GroupNorm(6, 6)\n",
    "# >>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n",
    "# >>> m = nn.GroupNorm(1, 6)\n",
    "# >>> # Activating the module\n",
    "# >>> output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "# The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. \n",
    "# The mean and standard-deviation are calculated separately over the each group. \n",
    "NetworkGN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.GroupNorm(3,6),\n",
    "    nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "    nn.Flatten(start_dim = 1),\n",
    "    nn.Linear(in_features = 12*4*4, out_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.GroupNorm(1, 120),\n",
    "    nn.Linear(in_features = 120, out_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features = 60, out_features = 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [128, 6, 24, 24]             156\n",
      "              ReLU-2           [128, 6, 24, 24]               0\n",
      "         MaxPool2d-3           [128, 6, 12, 12]               0\n",
      "         GroupNorm-4           [128, 6, 12, 12]              12\n",
      "            Conv2d-5            [128, 12, 8, 8]           1,812\n",
      "              ReLU-6            [128, 12, 8, 8]               0\n",
      "         MaxPool2d-7            [128, 12, 4, 4]               0\n",
      "           Flatten-8                 [128, 192]               0\n",
      "            Linear-9                 [128, 120]          23,160\n",
      "             ReLU-10                 [128, 120]               0\n",
      "        GroupNorm-11                 [128, 120]             240\n",
      "           Linear-12                  [128, 60]           7,260\n",
      "             ReLU-13                  [128, 60]               0\n",
      "           Linear-14                  [128, 10]             610\n",
      "================================================================\n",
      "Total params: 33,250\n",
      "Trainable params: 33,250\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 10.79\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 11.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkGN.to(device)\n",
    "summary(model, input_size=(1, 28, 28), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "cOe6m7forHCM"
   },
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mBOxsN4A7N8K"
   },
   "outputs": [],
   "source": [
    "# import modules to build RunBuilder and RunManager helper classes\n",
    "from collections  import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HSddFGL7zisa"
   },
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "\n",
    "        # tracking every epoch count, loss, accuracy, time\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "\n",
    "        # record model, loader and TensorBoard \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "\n",
    "    # record the count, hyper-param, model, loader of each run\n",
    "    # record sample images and network graph to TensorBoard  \n",
    "    def begin_run(self, run, network, loader):\n",
    "\n",
    "        self.run_start_time = time.time()\n",
    "\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment=f'-{run}')\n",
    "\n",
    "        images, labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "#         self.tb.add_image('images', grid)\n",
    "#         self.tb.add_graph(self.network, images)\n",
    "\n",
    "    # when run ends, close TensorBoard, zero epoch count\n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "\n",
    "    # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "\n",
    "    # \n",
    "    def end_epoch(self):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        # Record epoch loss and accuracy to TensorBoard \n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        # Record params to TensorBoard\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "\n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "\n",
    "        # Record hyper-params into 'results'\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "        # display epoch information and show progress\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "    # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, loss):\n",
    "        # multiply batch size so variety of batch sizes can be compared\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "    # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "    # save end results of all runs into csv, json for further a\n",
    "    def save(self, fileName):\n",
    "\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data, \n",
    "            orient = 'columns',\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1Rs-jD_Z-C-L"
   },
   "outputs": [],
   "source": [
    "# Run multiple networks through testing framework\n",
    "networks = {\n",
    "    'no_batch_norm': Network1,\n",
    "    'batch_norm': NetworkBN,\n",
    "    'layer_norm': NetworkLN,\n",
    "    'instance_norm': NetworkIN,\n",
    "    'group_norm': NetworkGN\n",
    "}\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "ZL-SM1wgUHql",
    "outputId": "521e2125-c967-4c92-c0c8-c9b5696be71f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501964</td>\n",
       "      <td>0.813250</td>\n",
       "      <td>104.005440</td>\n",
       "      <td>104.097855</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.357416</td>\n",
       "      <td>0.868633</td>\n",
       "      <td>101.602623</td>\n",
       "      <td>205.779470</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.320824</td>\n",
       "      <td>0.879733</td>\n",
       "      <td>102.224217</td>\n",
       "      <td>308.073121</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.299649</td>\n",
       "      <td>0.888950</td>\n",
       "      <td>101.741824</td>\n",
       "      <td>409.880468</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.283599</td>\n",
       "      <td>0.894933</td>\n",
       "      <td>100.488043</td>\n",
       "      <td>510.431862</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>0.978417</td>\n",
       "      <td>9.381324</td>\n",
       "      <td>153.329192</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>9.394866</td>\n",
       "      <td>162.814431</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.055642</td>\n",
       "      <td>0.979683</td>\n",
       "      <td>9.403691</td>\n",
       "      <td>172.311783</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.053220</td>\n",
       "      <td>0.979983</td>\n",
       "      <td>9.996461</td>\n",
       "      <td>182.390891</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.980550</td>\n",
       "      <td>9.407287</td>\n",
       "      <td>191.887085</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "0      1      1  0.501964  0.813250      104.005440    104.097855  0.0005   \n",
       "1      1      2  0.357416  0.868633      101.602623    205.779470  0.0005   \n",
       "2      1      3  0.320824  0.879733      102.224217    308.073121  0.0005   \n",
       "3      1      4  0.299649  0.888950      101.741824    409.880468  0.0005   \n",
       "4      1      5  0.283599  0.894933      100.488043    510.431862  0.0005   \n",
       "..   ...    ...       ...       ...             ...           ...     ...   \n",
       "195   10     16  0.058070  0.978417        9.381324    153.329192  0.0005   \n",
       "196   10     17  0.056952  0.978700        9.394866    162.814431  0.0005   \n",
       "197   10     18  0.055642  0.979683        9.403691    172.311783  0.0005   \n",
       "198   10     19  0.053220  0.979983        9.996461    182.390891  0.0005   \n",
       "199   10     20  0.051591  0.980550        9.407287    191.887085  0.0005   \n",
       "\n",
       "     batch_size  num_workers device trainset        network  \n",
       "0             2            1   cuda   normal  no_batch_norm  \n",
       "1             2            1   cuda   normal  no_batch_norm  \n",
       "2             2            1   cuda   normal  no_batch_norm  \n",
       "3             2            1   cuda   normal  no_batch_norm  \n",
       "4             2            1   cuda   normal  no_batch_norm  \n",
       "..          ...          ...    ...      ...            ...  \n",
       "195          32            1   cuda   normal     group_norm  \n",
       "196          32            1   cuda   normal     group_norm  \n",
       "197          32            1   cuda   normal     group_norm  \n",
       "198          32            1   cuda   normal     group_norm  \n",
       "199          32            1   cuda   normal     group_norm  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test different configurations\n",
    "# for every run [value] that is going to be used e.g [.001, .01] = two runs\n",
    "params = OrderedDict(\n",
    "    lr = [.0005],\n",
    "    batch_size = [2, 32],\n",
    "    num_workers = [1],\n",
    "    device = [\"cuda\"],\n",
    "    trainset = [\"normal\"],\n",
    "    # try all the values in the dict network1, network2\n",
    "    network = list(networks.keys())\n",
    ")\n",
    "m = RunManager()\n",
    "# active run or current run\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    # redefine the network\n",
    "    network = networks[run.network].to(device)\n",
    "    loader = DataLoader(trainsets[run.trainset], batch_size = run.batch_size, num_workers = run.num_workers, \n",
    "                       pin_memory = True) \n",
    "    optimizer = optim.Adam(network.parameters(), lr = run.lr) \n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(epochs):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            \n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            #7.9\n",
    "            #optimizer.zero_grad()\n",
    "            #8 sec\n",
    "            for p in network.parameters(): p.grad = None\n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update Weights\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>0.989967</td>\n",
       "      <td>11.541177</td>\n",
       "      <td>222.439402</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.989917</td>\n",
       "      <td>11.553787</td>\n",
       "      <td>234.079134</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.989750</td>\n",
       "      <td>11.535239</td>\n",
       "      <td>199.222934</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.989533</td>\n",
       "      <td>11.501659</td>\n",
       "      <td>210.810308</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.031225</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>11.610630</td>\n",
       "      <td>187.603899</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870528</td>\n",
       "      <td>0.671367</td>\n",
       "      <td>127.477228</td>\n",
       "      <td>1148.852623</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>0.663950</td>\n",
       "      <td>127.726983</td>\n",
       "      <td>510.655797</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933278</td>\n",
       "      <td>0.643617</td>\n",
       "      <td>127.398721</td>\n",
       "      <td>382.845248</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979345</td>\n",
       "      <td>0.634617</td>\n",
       "      <td>127.382731</td>\n",
       "      <td>255.363166</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.171205</td>\n",
       "      <td>0.577817</td>\n",
       "      <td>127.825136</td>\n",
       "      <td>127.893252</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "178    9     19  0.028089  0.989967       11.541177    222.439402  0.0005   \n",
       "179    9     20  0.027090  0.989917       11.553787    234.079134  0.0005   \n",
       "176    9     17  0.029152  0.989750       11.535239    199.222934  0.0005   \n",
       "177    9     18  0.028601  0.989533       11.501659    210.810308  0.0005   \n",
       "175    9     16  0.031225  0.988800       11.610630    187.603899  0.0005   \n",
       "..   ...    ...       ...       ...             ...           ...     ...   \n",
       "28     2      9  0.870528  0.671367      127.477228   1148.852623  0.0005   \n",
       "23     2      4  0.879747  0.663950      127.726983    510.655797  0.0005   \n",
       "22     2      3  0.933278  0.643617      127.398721    382.845248  0.0005   \n",
       "21     2      2  0.979345  0.634617      127.382731    255.363166  0.0005   \n",
       "20     2      1  1.171205  0.577817      127.825136    127.893252  0.0005   \n",
       "\n",
       "     batch_size  num_workers device trainset        network  \n",
       "178          32            1   cuda   normal  instance_norm  \n",
       "179          32            1   cuda   normal  instance_norm  \n",
       "176          32            1   cuda   normal  instance_norm  \n",
       "177          32            1   cuda   normal  instance_norm  \n",
       "175          32            1   cuda   normal  instance_norm  \n",
       "..          ...          ...    ...      ...            ...  \n",
       "28            2            1   cuda   normal     batch_norm  \n",
       "23            2            1   cuda   normal     batch_norm  \n",
       "22            2            1   cuda   normal     batch_norm  \n",
       "21            2            1   cuda   normal     batch_norm  \n",
       "20            2            1   cuda   normal     batch_norm  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>0.989967</td>\n",
       "      <td>11.541177</td>\n",
       "      <td>222.439402</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.989917</td>\n",
       "      <td>11.553787</td>\n",
       "      <td>234.079134</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.989750</td>\n",
       "      <td>11.535239</td>\n",
       "      <td>199.222934</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.989533</td>\n",
       "      <td>11.501659</td>\n",
       "      <td>210.810308</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.031225</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>11.610630</td>\n",
       "      <td>187.603899</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.988633</td>\n",
       "      <td>12.152803</td>\n",
       "      <td>175.903975</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>0.987717</td>\n",
       "      <td>11.533310</td>\n",
       "      <td>163.667644</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.036655</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>11.574715</td>\n",
       "      <td>152.053688</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.985850</td>\n",
       "      <td>11.514431</td>\n",
       "      <td>140.395772</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.040686</td>\n",
       "      <td>0.985283</td>\n",
       "      <td>11.526553</td>\n",
       "      <td>128.795341</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.984917</td>\n",
       "      <td>9.520932</td>\n",
       "      <td>192.043671</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.042889</td>\n",
       "      <td>0.984800</td>\n",
       "      <td>11.600348</td>\n",
       "      <td>117.188140</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.043563</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>9.450998</td>\n",
       "      <td>172.890299</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.046899</td>\n",
       "      <td>0.983517</td>\n",
       "      <td>11.563998</td>\n",
       "      <td>105.503401</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0.043355</td>\n",
       "      <td>0.983367</td>\n",
       "      <td>9.447998</td>\n",
       "      <td>182.436687</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.045808</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>9.429050</td>\n",
       "      <td>163.350328</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.982733</td>\n",
       "      <td>11.489270</td>\n",
       "      <td>93.851689</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>0.982117</td>\n",
       "      <td>9.434737</td>\n",
       "      <td>144.323762</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>9.419952</td>\n",
       "      <td>153.830726</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.053474</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>11.517577</td>\n",
       "      <td>82.282215</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>0.980650</td>\n",
       "      <td>9.454891</td>\n",
       "      <td>134.799242</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.980550</td>\n",
       "      <td>9.407287</td>\n",
       "      <td>191.887085</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.052599</td>\n",
       "      <td>0.980483</td>\n",
       "      <td>9.985198</td>\n",
       "      <td>125.246301</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.053220</td>\n",
       "      <td>0.979983</td>\n",
       "      <td>9.996461</td>\n",
       "      <td>182.390891</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.055642</td>\n",
       "      <td>0.979683</td>\n",
       "      <td>9.403691</td>\n",
       "      <td>172.311783</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.058160</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>11.565159</td>\n",
       "      <td>70.685301</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>9.394866</td>\n",
       "      <td>162.814431</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>0.978417</td>\n",
       "      <td>9.381324</td>\n",
       "      <td>153.329192</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.056217</td>\n",
       "      <td>0.978367</td>\n",
       "      <td>9.471505</td>\n",
       "      <td>115.171755</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.059965</td>\n",
       "      <td>0.977567</td>\n",
       "      <td>9.435142</td>\n",
       "      <td>105.615466</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.062730</td>\n",
       "      <td>0.976450</td>\n",
       "      <td>9.416074</td>\n",
       "      <td>143.852148</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.063543</td>\n",
       "      <td>0.976450</td>\n",
       "      <td>11.999762</td>\n",
       "      <td>59.037222</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>0.976317</td>\n",
       "      <td>9.473040</td>\n",
       "      <td>96.092550</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.063552</td>\n",
       "      <td>0.976083</td>\n",
       "      <td>9.459668</td>\n",
       "      <td>134.349622</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.063843</td>\n",
       "      <td>0.975650</td>\n",
       "      <td>9.465370</td>\n",
       "      <td>86.529346</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.066850</td>\n",
       "      <td>0.975133</td>\n",
       "      <td>9.413498</td>\n",
       "      <td>124.805276</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.067398</td>\n",
       "      <td>0.974517</td>\n",
       "      <td>9.454893</td>\n",
       "      <td>76.978438</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.068155</td>\n",
       "      <td>0.974333</td>\n",
       "      <td>9.406157</td>\n",
       "      <td>115.297853</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.069389</td>\n",
       "      <td>0.974050</td>\n",
       "      <td>11.646329</td>\n",
       "      <td>46.964524</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.071128</td>\n",
       "      <td>0.973500</td>\n",
       "      <td>9.477992</td>\n",
       "      <td>105.807524</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.073426</td>\n",
       "      <td>0.972583</td>\n",
       "      <td>9.405222</td>\n",
       "      <td>96.245017</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.071757</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>9.427411</td>\n",
       "      <td>67.429293</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.076383</td>\n",
       "      <td>0.971450</td>\n",
       "      <td>11.482200</td>\n",
       "      <td>35.231395</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.078045</td>\n",
       "      <td>0.971367</td>\n",
       "      <td>9.411229</td>\n",
       "      <td>86.760070</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075849</td>\n",
       "      <td>0.970850</td>\n",
       "      <td>9.466286</td>\n",
       "      <td>57.919288</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.081299</td>\n",
       "      <td>0.969867</td>\n",
       "      <td>9.424503</td>\n",
       "      <td>77.263819</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.084686</td>\n",
       "      <td>0.968567</td>\n",
       "      <td>9.442837</td>\n",
       "      <td>67.750863</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.081533</td>\n",
       "      <td>0.968433</td>\n",
       "      <td>9.505929</td>\n",
       "      <td>48.364251</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085725</td>\n",
       "      <td>0.967600</td>\n",
       "      <td>11.480918</td>\n",
       "      <td>23.668532</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086391</td>\n",
       "      <td>0.967017</td>\n",
       "      <td>9.512797</td>\n",
       "      <td>38.766143</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.088779</td>\n",
       "      <td>0.966617</td>\n",
       "      <td>10.010719</td>\n",
       "      <td>58.216520</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.093255</td>\n",
       "      <td>0.964850</td>\n",
       "      <td>9.435171</td>\n",
       "      <td>48.122223</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.092716</td>\n",
       "      <td>0.964717</td>\n",
       "      <td>9.421376</td>\n",
       "      <td>29.169429</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.092148</td>\n",
       "      <td>0.964533</td>\n",
       "      <td>9.542772</td>\n",
       "      <td>192.693601</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>9.435114</td>\n",
       "      <td>38.597381</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0.097105</td>\n",
       "      <td>0.962050</td>\n",
       "      <td>9.465199</td>\n",
       "      <td>173.485176</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0.097768</td>\n",
       "      <td>0.961883</td>\n",
       "      <td>9.530903</td>\n",
       "      <td>183.080228</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100387</td>\n",
       "      <td>0.961650</td>\n",
       "      <td>9.479156</td>\n",
       "      <td>19.665425</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0.098261</td>\n",
       "      <td>0.961467</td>\n",
       "      <td>9.563473</td>\n",
       "      <td>163.949641</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102188</td>\n",
       "      <td>0.960650</td>\n",
       "      <td>12.025872</td>\n",
       "      <td>12.095550</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.102908</td>\n",
       "      <td>0.960083</td>\n",
       "      <td>9.718426</td>\n",
       "      <td>154.316523</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104805</td>\n",
       "      <td>0.959567</td>\n",
       "      <td>9.442926</td>\n",
       "      <td>29.074697</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.107338</td>\n",
       "      <td>0.958467</td>\n",
       "      <td>9.501519</td>\n",
       "      <td>144.536433</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.107452</td>\n",
       "      <td>0.957950</td>\n",
       "      <td>9.551929</td>\n",
       "      <td>134.960186</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.112022</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>9.424711</td>\n",
       "      <td>19.543373</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.112486</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>9.499557</td>\n",
       "      <td>125.332335</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.113681</td>\n",
       "      <td>0.955383</td>\n",
       "      <td>9.440812</td>\n",
       "      <td>115.767111</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116887</td>\n",
       "      <td>0.954817</td>\n",
       "      <td>9.978734</td>\n",
       "      <td>10.104935</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.119888</td>\n",
       "      <td>0.954083</td>\n",
       "      <td>133.066746</td>\n",
       "      <td>2656.584407</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.117354</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>9.456628</td>\n",
       "      <td>106.251816</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.120817</td>\n",
       "      <td>0.952983</td>\n",
       "      <td>9.469155</td>\n",
       "      <td>96.722999</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.124381</td>\n",
       "      <td>0.952550</td>\n",
       "      <td>132.302099</td>\n",
       "      <td>2523.443487</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0.126819</td>\n",
       "      <td>0.951450</td>\n",
       "      <td>133.097505</td>\n",
       "      <td>2391.067240</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127402</td>\n",
       "      <td>0.950517</td>\n",
       "      <td>9.951669</td>\n",
       "      <td>10.027931</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125894</td>\n",
       "      <td>0.950417</td>\n",
       "      <td>9.483924</td>\n",
       "      <td>87.184310</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.128394</td>\n",
       "      <td>0.950100</td>\n",
       "      <td>9.484133</td>\n",
       "      <td>77.624789</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.134389</td>\n",
       "      <td>0.948900</td>\n",
       "      <td>133.120671</td>\n",
       "      <td>2257.893341</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.132992</td>\n",
       "      <td>0.948383</td>\n",
       "      <td>9.442029</td>\n",
       "      <td>68.075915</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.138730</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>123.373077</td>\n",
       "      <td>2471.378819</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.138743</td>\n",
       "      <td>0.947317</td>\n",
       "      <td>133.262655</td>\n",
       "      <td>2124.682172</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>123.437404</td>\n",
       "      <td>2347.920231</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.146646</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>123.143592</td>\n",
       "      <td>2224.384549</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.138430</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>9.547269</td>\n",
       "      <td>58.553735</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.944417</td>\n",
       "      <td>9.530880</td>\n",
       "      <td>48.925132</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.146266</td>\n",
       "      <td>0.944200</td>\n",
       "      <td>125.141507</td>\n",
       "      <td>2504.851756</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.147051</td>\n",
       "      <td>0.943667</td>\n",
       "      <td>132.849934</td>\n",
       "      <td>1991.332728</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.151483</td>\n",
       "      <td>0.943317</td>\n",
       "      <td>123.766007</td>\n",
       "      <td>2101.158220</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.148252</td>\n",
       "      <td>0.943317</td>\n",
       "      <td>125.189965</td>\n",
       "      <td>2379.641830</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.148737</td>\n",
       "      <td>0.942583</td>\n",
       "      <td>9.499350</td>\n",
       "      <td>39.326548</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.152125</td>\n",
       "      <td>0.941850</td>\n",
       "      <td>133.553682</td>\n",
       "      <td>1858.414448</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.154362</td>\n",
       "      <td>0.941600</td>\n",
       "      <td>123.060800</td>\n",
       "      <td>1977.298687</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.153636</td>\n",
       "      <td>0.941517</td>\n",
       "      <td>124.824728</td>\n",
       "      <td>2254.375493</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.158424</td>\n",
       "      <td>0.940617</td>\n",
       "      <td>123.324920</td>\n",
       "      <td>1854.151594</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.157326</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>132.805301</td>\n",
       "      <td>1724.779378</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.157239</td>\n",
       "      <td>0.939650</td>\n",
       "      <td>125.275851</td>\n",
       "      <td>2129.472105</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.156499</td>\n",
       "      <td>0.939517</td>\n",
       "      <td>10.337678</td>\n",
       "      <td>29.750474</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.160441</td>\n",
       "      <td>0.938583</td>\n",
       "      <td>125.177333</td>\n",
       "      <td>2004.130404</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.162760</td>\n",
       "      <td>0.938317</td>\n",
       "      <td>123.524925</td>\n",
       "      <td>1730.743119</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.167787</td>\n",
       "      <td>0.938250</td>\n",
       "      <td>123.610539</td>\n",
       "      <td>1607.125916</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.163182</td>\n",
       "      <td>0.938150</td>\n",
       "      <td>132.443552</td>\n",
       "      <td>1591.897417</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166052</td>\n",
       "      <td>0.936450</td>\n",
       "      <td>9.496757</td>\n",
       "      <td>19.346013</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.166446</td>\n",
       "      <td>0.936350</td>\n",
       "      <td>125.447843</td>\n",
       "      <td>1878.878697</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>0.934583</td>\n",
       "      <td>123.558538</td>\n",
       "      <td>1483.427345</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.171485</td>\n",
       "      <td>0.933883</td>\n",
       "      <td>125.188057</td>\n",
       "      <td>1753.360350</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.172430</td>\n",
       "      <td>0.933217</td>\n",
       "      <td>132.478563</td>\n",
       "      <td>1459.374822</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.180501</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>123.648590</td>\n",
       "      <td>1359.785808</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.175814</td>\n",
       "      <td>0.932850</td>\n",
       "      <td>125.116316</td>\n",
       "      <td>1628.088578</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.181628</td>\n",
       "      <td>0.930467</td>\n",
       "      <td>132.797485</td>\n",
       "      <td>1326.818257</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.187298</td>\n",
       "      <td>0.930300</td>\n",
       "      <td>123.570461</td>\n",
       "      <td>1236.050597</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.182385</td>\n",
       "      <td>0.930167</td>\n",
       "      <td>125.253870</td>\n",
       "      <td>1502.903483</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186147</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>9.702673</td>\n",
       "      <td>9.777871</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.189725</td>\n",
       "      <td>0.928300</td>\n",
       "      <td>132.519233</td>\n",
       "      <td>1193.933981</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.188743</td>\n",
       "      <td>0.927250</td>\n",
       "      <td>124.984431</td>\n",
       "      <td>1377.570916</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.195125</td>\n",
       "      <td>0.927050</td>\n",
       "      <td>123.543366</td>\n",
       "      <td>1112.393235</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.924383</td>\n",
       "      <td>132.641872</td>\n",
       "      <td>1061.335583</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.197467</td>\n",
       "      <td>0.924317</td>\n",
       "      <td>125.151045</td>\n",
       "      <td>1252.504393</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.206551</td>\n",
       "      <td>0.923133</td>\n",
       "      <td>123.505531</td>\n",
       "      <td>988.762645</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.205327</td>\n",
       "      <td>0.921633</td>\n",
       "      <td>125.280111</td>\n",
       "      <td>1127.275618</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.919100</td>\n",
       "      <td>125.027379</td>\n",
       "      <td>1001.910144</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.211491</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>132.581046</td>\n",
       "      <td>928.614762</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.215968</td>\n",
       "      <td>0.918967</td>\n",
       "      <td>123.466833</td>\n",
       "      <td>865.168372</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.226999</td>\n",
       "      <td>0.915367</td>\n",
       "      <td>123.643193</td>\n",
       "      <td>741.611197</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.224271</td>\n",
       "      <td>0.915167</td>\n",
       "      <td>125.056012</td>\n",
       "      <td>876.803031</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.225324</td>\n",
       "      <td>0.913467</td>\n",
       "      <td>132.360454</td>\n",
       "      <td>795.953664</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.236438</td>\n",
       "      <td>0.913250</td>\n",
       "      <td>100.657438</td>\n",
       "      <td>1620.837793</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.236522</td>\n",
       "      <td>0.913017</td>\n",
       "      <td>100.259965</td>\n",
       "      <td>1821.624621</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.237167</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>99.902772</td>\n",
       "      <td>2021.995448</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.236972</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>100.325087</td>\n",
       "      <td>1922.024058</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.238041</td>\n",
       "      <td>0.912717</td>\n",
       "      <td>100.376904</td>\n",
       "      <td>1721.288235</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.236063</td>\n",
       "      <td>0.911617</td>\n",
       "      <td>125.012394</td>\n",
       "      <td>751.667848</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.238044</td>\n",
       "      <td>0.911450</td>\n",
       "      <td>100.322547</td>\n",
       "      <td>1319.857795</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.237992</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>100.715679</td>\n",
       "      <td>1219.472464</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.240004</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>100.218387</td>\n",
       "      <td>1520.111450</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.241445</td>\n",
       "      <td>0.910367</td>\n",
       "      <td>123.616437</td>\n",
       "      <td>617.876371</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.240029</td>\n",
       "      <td>0.910267</td>\n",
       "      <td>99.900144</td>\n",
       "      <td>1419.826144</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.241193</td>\n",
       "      <td>0.909350</td>\n",
       "      <td>100.584167</td>\n",
       "      <td>1118.692733</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.240704</td>\n",
       "      <td>0.909250</td>\n",
       "      <td>132.592783</td>\n",
       "      <td>663.517014</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.247537</td>\n",
       "      <td>0.908283</td>\n",
       "      <td>102.650820</td>\n",
       "      <td>1018.042055</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.906167</td>\n",
       "      <td>100.964096</td>\n",
       "      <td>915.321031</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.252923</td>\n",
       "      <td>0.905767</td>\n",
       "      <td>125.172731</td>\n",
       "      <td>626.568396</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.258771</td>\n",
       "      <td>0.904583</td>\n",
       "      <td>123.354796</td>\n",
       "      <td>494.175111</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.258450</td>\n",
       "      <td>0.903867</td>\n",
       "      <td>101.443884</td>\n",
       "      <td>814.289690</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.258411</td>\n",
       "      <td>0.903383</td>\n",
       "      <td>9.651796</td>\n",
       "      <td>192.427073</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.259093</td>\n",
       "      <td>0.901967</td>\n",
       "      <td>132.576850</td>\n",
       "      <td>530.836762</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.264973</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>101.317605</td>\n",
       "      <td>712.784264</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0.261723</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>9.485402</td>\n",
       "      <td>182.683126</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0.270831</td>\n",
       "      <td>0.899033</td>\n",
       "      <td>9.387133</td>\n",
       "      <td>163.616963</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0.269431</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>9.409261</td>\n",
       "      <td>173.111754</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.270580</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>125.045716</td>\n",
       "      <td>501.309973</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.271917</td>\n",
       "      <td>0.898467</td>\n",
       "      <td>100.888806</td>\n",
       "      <td>611.393888</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.275648</td>\n",
       "      <td>0.896850</td>\n",
       "      <td>9.415888</td>\n",
       "      <td>154.149016</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282123</td>\n",
       "      <td>0.895450</td>\n",
       "      <td>123.573046</td>\n",
       "      <td>370.722218</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.283599</td>\n",
       "      <td>0.894933</td>\n",
       "      <td>100.488043</td>\n",
       "      <td>510.431862</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.281442</td>\n",
       "      <td>0.894267</td>\n",
       "      <td>9.546338</td>\n",
       "      <td>144.646336</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.287725</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>9.483280</td>\n",
       "      <td>135.018764</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.891833</td>\n",
       "      <td>132.105242</td>\n",
       "      <td>398.170365</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.295130</td>\n",
       "      <td>0.890233</td>\n",
       "      <td>125.063706</td>\n",
       "      <td>376.179906</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.294450</td>\n",
       "      <td>0.889483</td>\n",
       "      <td>9.475746</td>\n",
       "      <td>125.448545</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.299649</td>\n",
       "      <td>0.888950</td>\n",
       "      <td>101.741824</td>\n",
       "      <td>409.880468</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.302846</td>\n",
       "      <td>0.886250</td>\n",
       "      <td>9.435694</td>\n",
       "      <td>115.874756</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.311969</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>9.495887</td>\n",
       "      <td>106.348443</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.318107</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>123.256393</td>\n",
       "      <td>247.053110</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.880117</td>\n",
       "      <td>132.757443</td>\n",
       "      <td>265.973528</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.320824</td>\n",
       "      <td>0.879733</td>\n",
       "      <td>102.224217</td>\n",
       "      <td>308.073121</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.321604</td>\n",
       "      <td>0.879217</td>\n",
       "      <td>9.442910</td>\n",
       "      <td>96.769076</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.331771</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>125.076399</td>\n",
       "      <td>251.027926</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333062</td>\n",
       "      <td>0.875867</td>\n",
       "      <td>9.493145</td>\n",
       "      <td>87.226720</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.345273</td>\n",
       "      <td>0.871100</td>\n",
       "      <td>9.917897</td>\n",
       "      <td>77.654850</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.357416</td>\n",
       "      <td>0.868633</td>\n",
       "      <td>101.602623</td>\n",
       "      <td>205.779470</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.358250</td>\n",
       "      <td>0.866167</td>\n",
       "      <td>9.488028</td>\n",
       "      <td>67.645473</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.373966</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>9.414521</td>\n",
       "      <td>58.061165</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.392614</td>\n",
       "      <td>0.852833</td>\n",
       "      <td>9.432120</td>\n",
       "      <td>48.557345</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.418983</td>\n",
       "      <td>0.843817</td>\n",
       "      <td>9.423000</td>\n",
       "      <td>39.032021</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429439</td>\n",
       "      <td>0.841633</td>\n",
       "      <td>133.068873</td>\n",
       "      <td>133.134367</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>instance_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440402</td>\n",
       "      <td>0.837783</td>\n",
       "      <td>123.640288</td>\n",
       "      <td>123.705391</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>layer_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449645</td>\n",
       "      <td>0.834067</td>\n",
       "      <td>125.807395</td>\n",
       "      <td>125.873505</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>group_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.455337</td>\n",
       "      <td>0.828750</td>\n",
       "      <td>9.459464</td>\n",
       "      <td>29.518483</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501964</td>\n",
       "      <td>0.813250</td>\n",
       "      <td>104.005440</td>\n",
       "      <td>104.097855</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>no_batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509130</td>\n",
       "      <td>0.807067</td>\n",
       "      <td>9.446134</td>\n",
       "      <td>19.969215</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630221</td>\n",
       "      <td>0.767233</td>\n",
       "      <td>10.354187</td>\n",
       "      <td>10.429960</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.773690</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>126.964398</td>\n",
       "      <td>2427.458533</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.784231</td>\n",
       "      <td>0.700067</td>\n",
       "      <td>127.353667</td>\n",
       "      <td>2170.581031</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.800379</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>129.419541</td>\n",
       "      <td>2043.136961</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.806599</td>\n",
       "      <td>0.694650</td>\n",
       "      <td>127.518617</td>\n",
       "      <td>2555.076335</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.804308</td>\n",
       "      <td>0.690850</td>\n",
       "      <td>129.734677</td>\n",
       "      <td>2300.405396</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.817448</td>\n",
       "      <td>0.687483</td>\n",
       "      <td>127.588315</td>\n",
       "      <td>1021.299723</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.819798</td>\n",
       "      <td>0.685133</td>\n",
       "      <td>127.526950</td>\n",
       "      <td>893.630613</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.842053</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>127.310761</td>\n",
       "      <td>1913.636129</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.839238</td>\n",
       "      <td>0.682283</td>\n",
       "      <td>127.216470</td>\n",
       "      <td>1276.154767</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.827867</td>\n",
       "      <td>0.681317</td>\n",
       "      <td>127.816330</td>\n",
       "      <td>638.559270</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.824307</td>\n",
       "      <td>0.681117</td>\n",
       "      <td>127.494715</td>\n",
       "      <td>1403.744922</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.843212</td>\n",
       "      <td>0.680633</td>\n",
       "      <td>126.992948</td>\n",
       "      <td>1658.428497</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.855576</td>\n",
       "      <td>0.675783</td>\n",
       "      <td>127.518263</td>\n",
       "      <td>1531.351243</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.836406</td>\n",
       "      <td>0.674550</td>\n",
       "      <td>127.371776</td>\n",
       "      <td>766.019436</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.858578</td>\n",
       "      <td>0.674433</td>\n",
       "      <td>127.721335</td>\n",
       "      <td>1786.235401</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870528</td>\n",
       "      <td>0.671367</td>\n",
       "      <td>127.477228</td>\n",
       "      <td>1148.852623</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>0.663950</td>\n",
       "      <td>127.726983</td>\n",
       "      <td>510.655797</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933278</td>\n",
       "      <td>0.643617</td>\n",
       "      <td>127.398721</td>\n",
       "      <td>382.845248</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979345</td>\n",
       "      <td>0.634617</td>\n",
       "      <td>127.382731</td>\n",
       "      <td>255.363166</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.171205</td>\n",
       "      <td>0.577817</td>\n",
       "      <td>127.825136</td>\n",
       "      <td>127.893252</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "178    9     19  0.028089  0.989967       11.541177    222.439402  0.0005   \n",
       "179    9     20  0.027090  0.989917       11.553787    234.079134  0.0005   \n",
       "176    9     17  0.029152  0.989750       11.535239    199.222934  0.0005   \n",
       "177    9     18  0.028601  0.989533       11.501659    210.810308  0.0005   \n",
       "175    9     16  0.031225  0.988800       11.610630    187.603899  0.0005   \n",
       "174    9     15  0.032318  0.988633       12.152803    175.903975  0.0005   \n",
       "173    9     14  0.034830  0.987717       11.533310    163.667644  0.0005   \n",
       "172    9     13  0.036655  0.986667       11.574715    152.053688  0.0005   \n",
       "171    9     12  0.038866  0.985850       11.514431    140.395772  0.0005   \n",
       "170    9     11  0.040686  0.985283       11.526553    128.795341  0.0005   \n",
       "159    8     20  0.040755  0.984917        9.520932    192.043671  0.0005   \n",
       "169    9     10  0.042889  0.984800       11.600348    117.188140  0.0005   \n",
       "157    8     18  0.043563  0.984000        9.450998    172.890299  0.0005   \n",
       "168    9      9  0.046899  0.983517       11.563998    105.503401  0.0005   \n",
       "158    8     19  0.043355  0.983367        9.447998    182.436687  0.0005   \n",
       "156    8     17  0.045808  0.983033        9.429050    163.350328  0.0005   \n",
       "167    9      8  0.049383  0.982733       11.489270     93.851689  0.0005   \n",
       "154    8     15  0.047174  0.982117        9.434737    144.323762  0.0005   \n",
       "155    8     16  0.047887  0.981817        9.419952    153.830726  0.0005   \n",
       "166    9      7  0.053474  0.981000       11.517577     82.282215  0.0005   \n",
       "153    8     14  0.051372  0.980650        9.454891    134.799242  0.0005   \n",
       "199   10     20  0.051591  0.980550        9.407287    191.887085  0.0005   \n",
       "152    8     13  0.052599  0.980483        9.985198    125.246301  0.0005   \n",
       "198   10     19  0.053220  0.979983        9.996461    182.390891  0.0005   \n",
       "197   10     18  0.055642  0.979683        9.403691    172.311783  0.0005   \n",
       "165    9      6  0.058160  0.978717       11.565159     70.685301  0.0005   \n",
       "196   10     17  0.056952  0.978700        9.394866    162.814431  0.0005   \n",
       "195   10     16  0.058070  0.978417        9.381324    153.329192  0.0005   \n",
       "151    8     12  0.056217  0.978367        9.471505    115.171755  0.0005   \n",
       "150    8     11  0.059965  0.977567        9.435142    105.615466  0.0005   \n",
       "194   10     15  0.062730  0.976450        9.416074    143.852148  0.0005   \n",
       "164    9      5  0.063543  0.976450       11.999762     59.037222  0.0005   \n",
       "149    8     10  0.062226  0.976317        9.473040     96.092550  0.0005   \n",
       "193   10     14  0.063552  0.976083        9.459668    134.349622  0.0005   \n",
       "148    8      9  0.063843  0.975650        9.465370     86.529346  0.0005   \n",
       "192   10     13  0.066850  0.975133        9.413498    124.805276  0.0005   \n",
       "147    8      8  0.067398  0.974517        9.454893     76.978438  0.0005   \n",
       "191   10     12  0.068155  0.974333        9.406157    115.297853  0.0005   \n",
       "163    9      4  0.069389  0.974050       11.646329     46.964524  0.0005   \n",
       "190   10     11  0.071128  0.973500        9.477992    105.807524  0.0005   \n",
       "189   10     10  0.073426  0.972583        9.405222     96.245017  0.0005   \n",
       "146    8      7  0.071757  0.972500        9.427411     67.429293  0.0005   \n",
       "162    9      3  0.076383  0.971450       11.482200     35.231395  0.0005   \n",
       "188   10      9  0.078045  0.971367        9.411229     86.760070  0.0005   \n",
       "145    8      6  0.075849  0.970850        9.466286     57.919288  0.0005   \n",
       "187   10      8  0.081299  0.969867        9.424503     77.263819  0.0005   \n",
       "186   10      7  0.084686  0.968567        9.442837     67.750863  0.0005   \n",
       "144    8      5  0.081533  0.968433        9.505929     48.364251  0.0005   \n",
       "161    9      2  0.085725  0.967600       11.480918     23.668532  0.0005   \n",
       "143    8      4  0.086391  0.967017        9.512797     38.766143  0.0005   \n",
       "185   10      6  0.088779  0.966617       10.010719     58.216520  0.0005   \n",
       "184   10      5  0.093255  0.964850        9.435171     48.122223  0.0005   \n",
       "142    8      3  0.092716  0.964717        9.421376     29.169429  0.0005   \n",
       "119    6     20  0.092148  0.964533        9.542772    192.693601  0.0005   \n",
       "183   10      4  0.098488  0.962733        9.435114     38.597381  0.0005   \n",
       "117    6     18  0.097105  0.962050        9.465199    173.485176  0.0005   \n",
       "118    6     19  0.097768  0.961883        9.530903    183.080228  0.0005   \n",
       "141    8      2  0.100387  0.961650        9.479156     19.665425  0.0005   \n",
       "116    6     17  0.098261  0.961467        9.563473    163.949641  0.0005   \n",
       "160    9      1  0.102188  0.960650       12.025872     12.095550  0.0005   \n",
       "115    6     16  0.102908  0.960083        9.718426    154.316523  0.0005   \n",
       "182   10      3  0.104805  0.959567        9.442926     29.074697  0.0005   \n",
       "114    6     15  0.107338  0.958467        9.501519    144.536433  0.0005   \n",
       "113    6     14  0.107452  0.957950        9.551929    134.960186  0.0005   \n",
       "181   10      2  0.112022  0.956800        9.424711     19.543373  0.0005   \n",
       "112    6     13  0.112486  0.956000        9.499557    125.332335  0.0005   \n",
       "111    6     12  0.113681  0.955383        9.440812    115.767111  0.0005   \n",
       "140    8      1  0.116887  0.954817        9.978734     10.104935  0.0005   \n",
       "79     4     20  0.119888  0.954083      133.066746   2656.584407  0.0005   \n",
       "110    6     11  0.117354  0.954000        9.456628    106.251816  0.0005   \n",
       "109    6     10  0.120817  0.952983        9.469155     96.722999  0.0005   \n",
       "78     4     19  0.124381  0.952550      132.302099   2523.443487  0.0005   \n",
       "77     4     18  0.126819  0.951450      133.097505   2391.067240  0.0005   \n",
       "180   10      1  0.127402  0.950517        9.951669     10.027931  0.0005   \n",
       "108    6      9  0.125894  0.950417        9.483924     87.184310  0.0005   \n",
       "107    6      8  0.128394  0.950100        9.484133     77.624789  0.0005   \n",
       "76     4     17  0.134389  0.948900      133.120671   2257.893341  0.0005   \n",
       "106    6      7  0.132992  0.948383        9.442029     68.075915  0.0005   \n",
       "59     3     20  0.138730  0.948033      123.373077   2471.378819  0.0005   \n",
       "75     4     16  0.138743  0.947317      133.262655   2124.682172  0.0005   \n",
       "58     3     19  0.142857  0.946300      123.437404   2347.920231  0.0005   \n",
       "57     3     18  0.146646  0.945833      123.143592   2224.384549  0.0005   \n",
       "105    6      6  0.138430  0.945783        9.547269     58.553735  0.0005   \n",
       "104    6      5  0.144736  0.944417        9.530880     48.925132  0.0005   \n",
       "99     5     20  0.146266  0.944200      125.141507   2504.851756  0.0005   \n",
       "74     4     15  0.147051  0.943667      132.849934   1991.332728  0.0005   \n",
       "56     3     17  0.151483  0.943317      123.766007   2101.158220  0.0005   \n",
       "98     5     19  0.148252  0.943317      125.189965   2379.641830  0.0005   \n",
       "103    6      4  0.148737  0.942583        9.499350     39.326548  0.0005   \n",
       "73     4     14  0.152125  0.941850      133.553682   1858.414448  0.0005   \n",
       "55     3     16  0.154362  0.941600      123.060800   1977.298687  0.0005   \n",
       "97     5     18  0.153636  0.941517      124.824728   2254.375493  0.0005   \n",
       "54     3     15  0.158424  0.940617      123.324920   1854.151594  0.0005   \n",
       "72     4     13  0.157326  0.940500      132.805301   1724.779378  0.0005   \n",
       "96     5     17  0.157239  0.939650      125.275851   2129.472105  0.0005   \n",
       "102    6      3  0.156499  0.939517       10.337678     29.750474  0.0005   \n",
       "95     5     16  0.160441  0.938583      125.177333   2004.130404  0.0005   \n",
       "53     3     14  0.162760  0.938317      123.524925   1730.743119  0.0005   \n",
       "52     3     13  0.167787  0.938250      123.610539   1607.125916  0.0005   \n",
       "71     4     12  0.163182  0.938150      132.443552   1591.897417  0.0005   \n",
       "101    6      2  0.166052  0.936450        9.496757     19.346013  0.0005   \n",
       "94     5     15  0.166446  0.936350      125.447843   1878.878697  0.0005   \n",
       "51     3     12  0.173831  0.934583      123.558538   1483.427345  0.0005   \n",
       "93     5     14  0.171485  0.933883      125.188057   1753.360350  0.0005   \n",
       "70     4     11  0.172430  0.933217      132.478563   1459.374822  0.0005   \n",
       "50     3     11  0.180501  0.932900      123.648590   1359.785808  0.0005   \n",
       "92     5     13  0.175814  0.932850      125.116316   1628.088578  0.0005   \n",
       "69     4     10  0.181628  0.930467      132.797485   1326.818257  0.0005   \n",
       "49     3     10  0.187298  0.930300      123.570461   1236.050597  0.0005   \n",
       "91     5     12  0.182385  0.930167      125.253870   1502.903483  0.0005   \n",
       "100    6      1  0.186147  0.929900        9.702673      9.777871  0.0005   \n",
       "68     4      9  0.189725  0.928300      132.519233   1193.933981  0.0005   \n",
       "90     5     11  0.188743  0.927250      124.984431   1377.570916  0.0005   \n",
       "48     3      9  0.195125  0.927050      123.543366   1112.393235  0.0005   \n",
       "67     4      8  0.199900  0.924383      132.641872   1061.335583  0.0005   \n",
       "89     5     10  0.197467  0.924317      125.151045   1252.504393  0.0005   \n",
       "47     3      8  0.206551  0.923133      123.505531    988.762645  0.0005   \n",
       "88     5      9  0.205327  0.921633      125.280111   1127.275618  0.0005   \n",
       "87     5      8  0.213870  0.919100      125.027379   1001.910144  0.0005   \n",
       "66     4      7  0.211491  0.919000      132.581046    928.614762  0.0005   \n",
       "46     3      7  0.215968  0.918967      123.466833    865.168372  0.0005   \n",
       "45     3      6  0.226999  0.915367      123.643193    741.611197  0.0005   \n",
       "86     5      7  0.224271  0.915167      125.056012    876.803031  0.0005   \n",
       "65     4      6  0.225324  0.913467      132.360454    795.953664  0.0005   \n",
       "15     1     16  0.236438  0.913250      100.657438   1620.837793  0.0005   \n",
       "17     1     18  0.236522  0.913017      100.259965   1821.624621  0.0005   \n",
       "19     1     20  0.237167  0.912800       99.902772   2021.995448  0.0005   \n",
       "18     1     19  0.236972  0.912750      100.325087   1922.024058  0.0005   \n",
       "16     1     17  0.238041  0.912717      100.376904   1721.288235  0.0005   \n",
       "85     5      6  0.236063  0.911617      125.012394    751.667848  0.0005   \n",
       "12     1     13  0.238044  0.911450      100.322547   1319.857795  0.0005   \n",
       "11     1     12  0.237992  0.911017      100.715679   1219.472464  0.0005   \n",
       "14     1     15  0.240004  0.910400      100.218387   1520.111450  0.0005   \n",
       "44     3      5  0.241445  0.910367      123.616437    617.876371  0.0005   \n",
       "13     1     14  0.240029  0.910267       99.900144   1419.826144  0.0005   \n",
       "10     1     11  0.241193  0.909350      100.584167   1118.692733  0.0005   \n",
       "64     4      5  0.240704  0.909250      132.592783    663.517014  0.0005   \n",
       "9      1     10  0.247537  0.908283      102.650820   1018.042055  0.0005   \n",
       "8      1      9  0.253000  0.906167      100.964096    915.321031  0.0005   \n",
       "84     5      5  0.252923  0.905767      125.172731    626.568396  0.0005   \n",
       "43     3      4  0.258771  0.904583      123.354796    494.175111  0.0005   \n",
       "7      1      8  0.258450  0.903867      101.443884    814.289690  0.0005   \n",
       "139    7     20  0.258411  0.903383        9.651796    192.427073  0.0005   \n",
       "63     4      4  0.259093  0.901967      132.576850    530.836762  0.0005   \n",
       "6      1      7  0.264973  0.901800      101.317605    712.784264  0.0005   \n",
       "138    7     19  0.261723  0.901767        9.485402    182.683126  0.0005   \n",
       "136    7     17  0.270831  0.899033        9.387133    163.616963  0.0005   \n",
       "137    7     18  0.269431  0.898700        9.409261    173.111754  0.0005   \n",
       "83     5      4  0.270580  0.898600      125.045716    501.309973  0.0005   \n",
       "5      1      6  0.271917  0.898467      100.888806    611.393888  0.0005   \n",
       "135    7     16  0.275648  0.896850        9.415888    154.149016  0.0005   \n",
       "42     3      3  0.282123  0.895450      123.573046    370.722218  0.0005   \n",
       "4      1      5  0.283599  0.894933      100.488043    510.431862  0.0005   \n",
       "134    7     15  0.281442  0.894267        9.546338    144.646336  0.0005   \n",
       "133    7     14  0.287725  0.892400        9.483280    135.018764  0.0005   \n",
       "62     4      3  0.285600  0.891833      132.105242    398.170365  0.0005   \n",
       "82     5      3  0.295130  0.890233      125.063706    376.179906  0.0005   \n",
       "132    7     13  0.294450  0.889483        9.475746    125.448545  0.0005   \n",
       "3      1      4  0.299649  0.888950      101.741824    409.880468  0.0005   \n",
       "131    7     12  0.302846  0.886250        9.435694    115.874756  0.0005   \n",
       "130    7     11  0.311969  0.882883        9.495887    106.348443  0.0005   \n",
       "41     3      2  0.318107  0.882617      123.256393    247.053110  0.0005   \n",
       "61     4      2  0.321991  0.880117      132.757443    265.973528  0.0005   \n",
       "2      1      3  0.320824  0.879733      102.224217    308.073121  0.0005   \n",
       "129    7     10  0.321604  0.879217        9.442910     96.769076  0.0005   \n",
       "81     5      2  0.331771  0.877033      125.076399    251.027926  0.0005   \n",
       "128    7      9  0.333062  0.875867        9.493145     87.226720  0.0005   \n",
       "127    7      8  0.345273  0.871100        9.917897     77.654850  0.0005   \n",
       "1      1      2  0.357416  0.868633      101.602623    205.779470  0.0005   \n",
       "126    7      7  0.358250  0.866167        9.488028     67.645473  0.0005   \n",
       "125    7      6  0.373966  0.860317        9.414521     58.061165  0.0005   \n",
       "124    7      5  0.392614  0.852833        9.432120     48.557345  0.0005   \n",
       "123    7      4  0.418983  0.843817        9.423000     39.032021  0.0005   \n",
       "60     4      1  0.429439  0.841633      133.068873    133.134367  0.0005   \n",
       "40     3      1  0.440402  0.837783      123.640288    123.705391  0.0005   \n",
       "80     5      1  0.449645  0.834067      125.807395    125.873505  0.0005   \n",
       "122    7      3  0.455337  0.828750        9.459464     29.518483  0.0005   \n",
       "0      1      1  0.501964  0.813250      104.005440    104.097855  0.0005   \n",
       "121    7      2  0.509130  0.807067        9.446134     19.969215  0.0005   \n",
       "120    7      1  0.630221  0.767233       10.354187     10.429960  0.0005   \n",
       "38     2     19  0.773690  0.700400      126.964398   2427.458533  0.0005   \n",
       "36     2     17  0.784231  0.700067      127.353667   2170.581031  0.0005   \n",
       "35     2     16  0.800379  0.696700      129.419541   2043.136961  0.0005   \n",
       "39     2     20  0.806599  0.694650      127.518617   2555.076335  0.0005   \n",
       "37     2     18  0.804308  0.690850      129.734677   2300.405396  0.0005   \n",
       "27     2      8  0.817448  0.687483      127.588315   1021.299723  0.0005   \n",
       "26     2      7  0.819798  0.685133      127.526950    893.630613  0.0005   \n",
       "34     2     15  0.842053  0.683000      127.310761   1913.636129  0.0005   \n",
       "29     2     10  0.839238  0.682283      127.216470   1276.154767  0.0005   \n",
       "24     2      5  0.827867  0.681317      127.816330    638.559270  0.0005   \n",
       "30     2     11  0.824307  0.681117      127.494715   1403.744922  0.0005   \n",
       "32     2     13  0.843212  0.680633      126.992948   1658.428497  0.0005   \n",
       "31     2     12  0.855576  0.675783      127.518263   1531.351243  0.0005   \n",
       "25     2      6  0.836406  0.674550      127.371776    766.019436  0.0005   \n",
       "33     2     14  0.858578  0.674433      127.721335   1786.235401  0.0005   \n",
       "28     2      9  0.870528  0.671367      127.477228   1148.852623  0.0005   \n",
       "23     2      4  0.879747  0.663950      127.726983    510.655797  0.0005   \n",
       "22     2      3  0.933278  0.643617      127.398721    382.845248  0.0005   \n",
       "21     2      2  0.979345  0.634617      127.382731    255.363166  0.0005   \n",
       "20     2      1  1.171205  0.577817      127.825136    127.893252  0.0005   \n",
       "\n",
       "     batch_size  num_workers device trainset        network  \n",
       "178          32            1   cuda   normal  instance_norm  \n",
       "179          32            1   cuda   normal  instance_norm  \n",
       "176          32            1   cuda   normal  instance_norm  \n",
       "177          32            1   cuda   normal  instance_norm  \n",
       "175          32            1   cuda   normal  instance_norm  \n",
       "174          32            1   cuda   normal  instance_norm  \n",
       "173          32            1   cuda   normal  instance_norm  \n",
       "172          32            1   cuda   normal  instance_norm  \n",
       "171          32            1   cuda   normal  instance_norm  \n",
       "170          32            1   cuda   normal  instance_norm  \n",
       "159          32            1   cuda   normal     layer_norm  \n",
       "169          32            1   cuda   normal  instance_norm  \n",
       "157          32            1   cuda   normal     layer_norm  \n",
       "168          32            1   cuda   normal  instance_norm  \n",
       "158          32            1   cuda   normal     layer_norm  \n",
       "156          32            1   cuda   normal     layer_norm  \n",
       "167          32            1   cuda   normal  instance_norm  \n",
       "154          32            1   cuda   normal     layer_norm  \n",
       "155          32            1   cuda   normal     layer_norm  \n",
       "166          32            1   cuda   normal  instance_norm  \n",
       "153          32            1   cuda   normal     layer_norm  \n",
       "199          32            1   cuda   normal     group_norm  \n",
       "152          32            1   cuda   normal     layer_norm  \n",
       "198          32            1   cuda   normal     group_norm  \n",
       "197          32            1   cuda   normal     group_norm  \n",
       "165          32            1   cuda   normal  instance_norm  \n",
       "196          32            1   cuda   normal     group_norm  \n",
       "195          32            1   cuda   normal     group_norm  \n",
       "151          32            1   cuda   normal     layer_norm  \n",
       "150          32            1   cuda   normal     layer_norm  \n",
       "194          32            1   cuda   normal     group_norm  \n",
       "164          32            1   cuda   normal  instance_norm  \n",
       "149          32            1   cuda   normal     layer_norm  \n",
       "193          32            1   cuda   normal     group_norm  \n",
       "148          32            1   cuda   normal     layer_norm  \n",
       "192          32            1   cuda   normal     group_norm  \n",
       "147          32            1   cuda   normal     layer_norm  \n",
       "191          32            1   cuda   normal     group_norm  \n",
       "163          32            1   cuda   normal  instance_norm  \n",
       "190          32            1   cuda   normal     group_norm  \n",
       "189          32            1   cuda   normal     group_norm  \n",
       "146          32            1   cuda   normal     layer_norm  \n",
       "162          32            1   cuda   normal  instance_norm  \n",
       "188          32            1   cuda   normal     group_norm  \n",
       "145          32            1   cuda   normal     layer_norm  \n",
       "187          32            1   cuda   normal     group_norm  \n",
       "186          32            1   cuda   normal     group_norm  \n",
       "144          32            1   cuda   normal     layer_norm  \n",
       "161          32            1   cuda   normal  instance_norm  \n",
       "143          32            1   cuda   normal     layer_norm  \n",
       "185          32            1   cuda   normal     group_norm  \n",
       "184          32            1   cuda   normal     group_norm  \n",
       "142          32            1   cuda   normal     layer_norm  \n",
       "119          32            1   cuda   normal  no_batch_norm  \n",
       "183          32            1   cuda   normal     group_norm  \n",
       "117          32            1   cuda   normal  no_batch_norm  \n",
       "118          32            1   cuda   normal  no_batch_norm  \n",
       "141          32            1   cuda   normal     layer_norm  \n",
       "116          32            1   cuda   normal  no_batch_norm  \n",
       "160          32            1   cuda   normal  instance_norm  \n",
       "115          32            1   cuda   normal  no_batch_norm  \n",
       "182          32            1   cuda   normal     group_norm  \n",
       "114          32            1   cuda   normal  no_batch_norm  \n",
       "113          32            1   cuda   normal  no_batch_norm  \n",
       "181          32            1   cuda   normal     group_norm  \n",
       "112          32            1   cuda   normal  no_batch_norm  \n",
       "111          32            1   cuda   normal  no_batch_norm  \n",
       "140          32            1   cuda   normal     layer_norm  \n",
       "79            2            1   cuda   normal  instance_norm  \n",
       "110          32            1   cuda   normal  no_batch_norm  \n",
       "109          32            1   cuda   normal  no_batch_norm  \n",
       "78            2            1   cuda   normal  instance_norm  \n",
       "77            2            1   cuda   normal  instance_norm  \n",
       "180          32            1   cuda   normal     group_norm  \n",
       "108          32            1   cuda   normal  no_batch_norm  \n",
       "107          32            1   cuda   normal  no_batch_norm  \n",
       "76            2            1   cuda   normal  instance_norm  \n",
       "106          32            1   cuda   normal  no_batch_norm  \n",
       "59            2            1   cuda   normal     layer_norm  \n",
       "75            2            1   cuda   normal  instance_norm  \n",
       "58            2            1   cuda   normal     layer_norm  \n",
       "57            2            1   cuda   normal     layer_norm  \n",
       "105          32            1   cuda   normal  no_batch_norm  \n",
       "104          32            1   cuda   normal  no_batch_norm  \n",
       "99            2            1   cuda   normal     group_norm  \n",
       "74            2            1   cuda   normal  instance_norm  \n",
       "56            2            1   cuda   normal     layer_norm  \n",
       "98            2            1   cuda   normal     group_norm  \n",
       "103          32            1   cuda   normal  no_batch_norm  \n",
       "73            2            1   cuda   normal  instance_norm  \n",
       "55            2            1   cuda   normal     layer_norm  \n",
       "97            2            1   cuda   normal     group_norm  \n",
       "54            2            1   cuda   normal     layer_norm  \n",
       "72            2            1   cuda   normal  instance_norm  \n",
       "96            2            1   cuda   normal     group_norm  \n",
       "102          32            1   cuda   normal  no_batch_norm  \n",
       "95            2            1   cuda   normal     group_norm  \n",
       "53            2            1   cuda   normal     layer_norm  \n",
       "52            2            1   cuda   normal     layer_norm  \n",
       "71            2            1   cuda   normal  instance_norm  \n",
       "101          32            1   cuda   normal  no_batch_norm  \n",
       "94            2            1   cuda   normal     group_norm  \n",
       "51            2            1   cuda   normal     layer_norm  \n",
       "93            2            1   cuda   normal     group_norm  \n",
       "70            2            1   cuda   normal  instance_norm  \n",
       "50            2            1   cuda   normal     layer_norm  \n",
       "92            2            1   cuda   normal     group_norm  \n",
       "69            2            1   cuda   normal  instance_norm  \n",
       "49            2            1   cuda   normal     layer_norm  \n",
       "91            2            1   cuda   normal     group_norm  \n",
       "100          32            1   cuda   normal  no_batch_norm  \n",
       "68            2            1   cuda   normal  instance_norm  \n",
       "90            2            1   cuda   normal     group_norm  \n",
       "48            2            1   cuda   normal     layer_norm  \n",
       "67            2            1   cuda   normal  instance_norm  \n",
       "89            2            1   cuda   normal     group_norm  \n",
       "47            2            1   cuda   normal     layer_norm  \n",
       "88            2            1   cuda   normal     group_norm  \n",
       "87            2            1   cuda   normal     group_norm  \n",
       "66            2            1   cuda   normal  instance_norm  \n",
       "46            2            1   cuda   normal     layer_norm  \n",
       "45            2            1   cuda   normal     layer_norm  \n",
       "86            2            1   cuda   normal     group_norm  \n",
       "65            2            1   cuda   normal  instance_norm  \n",
       "15            2            1   cuda   normal  no_batch_norm  \n",
       "17            2            1   cuda   normal  no_batch_norm  \n",
       "19            2            1   cuda   normal  no_batch_norm  \n",
       "18            2            1   cuda   normal  no_batch_norm  \n",
       "16            2            1   cuda   normal  no_batch_norm  \n",
       "85            2            1   cuda   normal     group_norm  \n",
       "12            2            1   cuda   normal  no_batch_norm  \n",
       "11            2            1   cuda   normal  no_batch_norm  \n",
       "14            2            1   cuda   normal  no_batch_norm  \n",
       "44            2            1   cuda   normal     layer_norm  \n",
       "13            2            1   cuda   normal  no_batch_norm  \n",
       "10            2            1   cuda   normal  no_batch_norm  \n",
       "64            2            1   cuda   normal  instance_norm  \n",
       "9             2            1   cuda   normal  no_batch_norm  \n",
       "8             2            1   cuda   normal  no_batch_norm  \n",
       "84            2            1   cuda   normal     group_norm  \n",
       "43            2            1   cuda   normal     layer_norm  \n",
       "7             2            1   cuda   normal  no_batch_norm  \n",
       "139          32            1   cuda   normal     batch_norm  \n",
       "63            2            1   cuda   normal  instance_norm  \n",
       "6             2            1   cuda   normal  no_batch_norm  \n",
       "138          32            1   cuda   normal     batch_norm  \n",
       "136          32            1   cuda   normal     batch_norm  \n",
       "137          32            1   cuda   normal     batch_norm  \n",
       "83            2            1   cuda   normal     group_norm  \n",
       "5             2            1   cuda   normal  no_batch_norm  \n",
       "135          32            1   cuda   normal     batch_norm  \n",
       "42            2            1   cuda   normal     layer_norm  \n",
       "4             2            1   cuda   normal  no_batch_norm  \n",
       "134          32            1   cuda   normal     batch_norm  \n",
       "133          32            1   cuda   normal     batch_norm  \n",
       "62            2            1   cuda   normal  instance_norm  \n",
       "82            2            1   cuda   normal     group_norm  \n",
       "132          32            1   cuda   normal     batch_norm  \n",
       "3             2            1   cuda   normal  no_batch_norm  \n",
       "131          32            1   cuda   normal     batch_norm  \n",
       "130          32            1   cuda   normal     batch_norm  \n",
       "41            2            1   cuda   normal     layer_norm  \n",
       "61            2            1   cuda   normal  instance_norm  \n",
       "2             2            1   cuda   normal  no_batch_norm  \n",
       "129          32            1   cuda   normal     batch_norm  \n",
       "81            2            1   cuda   normal     group_norm  \n",
       "128          32            1   cuda   normal     batch_norm  \n",
       "127          32            1   cuda   normal     batch_norm  \n",
       "1             2            1   cuda   normal  no_batch_norm  \n",
       "126          32            1   cuda   normal     batch_norm  \n",
       "125          32            1   cuda   normal     batch_norm  \n",
       "124          32            1   cuda   normal     batch_norm  \n",
       "123          32            1   cuda   normal     batch_norm  \n",
       "60            2            1   cuda   normal  instance_norm  \n",
       "40            2            1   cuda   normal     layer_norm  \n",
       "80            2            1   cuda   normal     group_norm  \n",
       "122          32            1   cuda   normal     batch_norm  \n",
       "0             2            1   cuda   normal  no_batch_norm  \n",
       "121          32            1   cuda   normal     batch_norm  \n",
       "120          32            1   cuda   normal     batch_norm  \n",
       "38            2            1   cuda   normal     batch_norm  \n",
       "36            2            1   cuda   normal     batch_norm  \n",
       "35            2            1   cuda   normal     batch_norm  \n",
       "39            2            1   cuda   normal     batch_norm  \n",
       "37            2            1   cuda   normal     batch_norm  \n",
       "27            2            1   cuda   normal     batch_norm  \n",
       "26            2            1   cuda   normal     batch_norm  \n",
       "34            2            1   cuda   normal     batch_norm  \n",
       "29            2            1   cuda   normal     batch_norm  \n",
       "24            2            1   cuda   normal     batch_norm  \n",
       "30            2            1   cuda   normal     batch_norm  \n",
       "32            2            1   cuda   normal     batch_norm  \n",
       "31            2            1   cuda   normal     batch_norm  \n",
       "25            2            1   cuda   normal     batch_norm  \n",
       "33            2            1   cuda   normal     batch_norm  \n",
       "28            2            1   cuda   normal     batch_norm  \n",
       "23            2            1   cuda   normal     batch_norm  \n",
       "22            2            1   cuda   normal     batch_norm  \n",
       "21            2            1   cuda   normal     batch_norm  \n",
       "20            2            1   cuda   normal     batch_norm  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3WJ2b7t-Ibdv"
   },
   "outputs": [],
   "source": [
    "# helper function to calculate all predictions of train set\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds),\n",
    "            dim = 0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SfSculOmI5Y3"
   },
   "outputs": [],
   "source": [
    "# bigger batch size since we only do FP\n",
    "#prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=1000)\n",
    "#train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "3Nfb8Z3aKz47",
    "outputId": "d7432c9d-25ba-4691-8546-cc6c1d90eec2"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "Cjmpnd7jeSvc",
    "outputId": "a7d62f30-ea10-4016-eeae-a17fcaee0184"
   },
   "outputs": [],
   "source": [
    "# use scikitplot to plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "\n",
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "_iMsrhFiHyLu",
    "outputId": "112e1d80-00cf-49d4-eb2c-6d2dd3576741"
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(train_set.targets,train_preds.argmax(dim=1), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "guThaIpBMl8J",
    "outputId": "69654f5b-aaeb-4ae7-8cfa-ab663c5b7ed7"
   },
   "outputs": [],
   "source": [
    "# use ngrok to display TensorBoard on Colab\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXIf3ARdMsFm"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './runs'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YerFMXaZU2WT"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ui9bGasKU7Qj",
    "outputId": "83898702-48b1-4b22-89a9-e39a89f48368"
   },
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIm37f1QVBIo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch Tutorial Basic v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
